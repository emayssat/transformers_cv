{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1L0RAspJNYSjP6chXPvGprxbiVvSrFXE4?usp=sharing)"
      ],
      "metadata": {
        "id": "ymBQmMWbETSc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dbGaH2V1Z09"
      },
      "source": [
        "# Generating images from text"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on HuggingFace notebook (https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion.ipynb)"
      ],
      "metadata": {
        "id": "Mb661qTE3PBg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qypI8IrW1HOm"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate an image"
      ],
      "metadata": {
        "id": "X-TAtzRH8rFM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6flj14B1fmf"
      },
      "outputs": [],
      "source": [
        "!pip install diffusers==0.2.4\n",
        "!pip install transformers scipy ftfy datasets\n",
        "!pip install \"ipywidgets>=7,<8\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-EKSga-a3aLf"
      },
      "outputs": [],
      "source": [
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()\n",
        "\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ajY3qwGK4x0J"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from diffusers import StableDiffusionPipeline\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# make sure you're logged in with `huggingface-cli login`\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", revision=\"fp16\", torch_dtype=torch.float16, use_auth_token=True).to(device) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YL8K5lls5g2o"
      },
      "outputs": [],
      "source": [
        "from torch import autocast\n",
        "\n",
        "prompt = \"A headshot of a man in his twenties with short dark hair\"\n",
        "with autocast(\"cuda\"):\n",
        "  image = pipe(prompt)[\"sample\"][0]\n",
        "\n",
        "image.save(f\"astronaut_rides_horse.png\")\n",
        "image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJLsvKlD6nXf"
      },
      "source": [
        "## Keep generating the same image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bToSIJXg6hXi"
      },
      "outputs": [],
      "source": [
        "generator = torch.Generator(device).manual_seed(1)\n",
        "\n",
        "with autocast('cuda'):\n",
        "  image = pipe(prompt, generator=generator)['sample'][0]\n",
        "\n",
        "image"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate a grid of images"
      ],
      "metadata": {
        "id": "Jz8ljQav8gXa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NkHydShd5X8Y"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "def image_grid(imgs, rows, cols):\n",
        "    assert len(imgs) == rows*cols\n",
        "\n",
        "    w, h = imgs[0].size\n",
        "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
        "    grid_w, grid_h = grid.size\n",
        "    \n",
        "    for i, img in enumerate(imgs):\n",
        "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
        "    return grid\n",
        "\n",
        "num_cols = 2\n",
        "num_rows = 2\n",
        "\n",
        "prompt = [prompt] * num_cols\n",
        "\n",
        "all_images = []\n",
        "for i in range(num_rows):\n",
        "  with autocast(\"cuda\"):\n",
        "    images = pipe(prompt)[\"sample\"]\n",
        "  all_images.extend(images)\n",
        "\n",
        "grid = image_grid(all_images, rows=num_rows, cols=num_cols)\n",
        "grid"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ayGh1ECI5bcc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imrEPOXXxNI_"
      },
      "source": [
        "# Tokenizers for Text"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Working with the Hugging Face library"
      ],
      "metadata": {
        "id": "ncJCvimR5-fQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We want to use the same weights for our model and tokenizer. How can we use the bert uncased checkpoint ('bert-base-uncased') for our tokenizer.**"
      ],
      "metadata": {
        "id": "g2rS5c636FvV"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1_Ha4SBsTvx"
      },
      "source": [
        "from transformers import AutoTokenizer\n",
        "checkpoint = 'bert-base-uncased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How can we determine how large the vocabulary is?**"
      ],
      "metadata": {
        "id": "zVn1o47e6shq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.vocab)\n",
        "print(f'The vocabulary size is {len(tokenizer.vocab)}')"
      ],
      "metadata": {
        "id": "MpB6OB_0BzyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Convert the following sentence into**\n",
        "1. Tokens\n",
        "2. Numerical IDs"
      ],
      "metadata": {
        "id": "AJXuMGZ37EmC"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyuA_C-qvFbe"
      },
      "source": [
        "sentence = 'I like NLP'\n",
        "print(sentence)\n",
        "tokens = tokenizer.tokenize(sentence)\n",
        "print(tokens)\n",
        "ids = tokenizer.encode(sentence)\n",
        "print(ids)\n",
        "print(tokenizer.decode(ids))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'{tokenizer.cls_token} -> {tokenizer.cls_token_id}')\n",
        "print(f'{tokenizer.sep_token} -> {tokenizer.sep_token_id}')"
      ],
      "metadata": {
        "id": "pRyotcZ913ai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'ðŸ˜€' in tokenizer.vocab"
      ],
      "metadata": {
        "id": "46StCEdk1mTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = 'I like NLPðŸ˜€'\n",
        "tokenizer.tokenize(sentence)"
      ],
      "metadata": {
        "id": "YFLZsLbf0BWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eu6Ilp-AajuT"
      },
      "source": [
        "first_sentence = 'I like NLP.'\n",
        "second_sentence = 'What about you?'\n",
        "input = tokenizer(first_sentence, second_sentence, return_tensors='pt')\n",
        "input"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input['input_ids']"
      ],
      "metadata": {
        "id": "YVhbhgLqBwvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input['token_type_ids']"
      ],
      "metadata": {
        "id": "OYZvoKtNDAKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7b1ClWqbLY1"
      },
      "source": [
        "input['attention_mask']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "first_sentence = 'I like NLP.'\n",
        "second_sentence = 'What are your thoughts on the subject?'\n",
        "input = tokenizer([first_sentence, second_sentence], padding=True, return_tensors='pt')\n",
        "input['attention_mask']"
      ],
      "metadata": {
        "id": "gaJEgVN6NaOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text classification - IMDB Dataset"
      ],
      "metadata": {
        "id": "m8ptc7LP6ZxD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Datasets library"
      ],
      "metadata": {
        "id": "1FfQ14kh6gRZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2ko7M5G54Y9"
      },
      "outputs": [],
      "source": [
        "from datasets import list_datasets\n",
        "list_datasets()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "imdb = load_dataset(\"imdb\")\n",
        "imdb"
      ],
      "metadata": {
        "id": "dzQufDiu7OKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Similar to a python dictionary, where each key corresponds to a different split"
      ],
      "metadata": {
        "id": "mzk6qYbd7yEH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "imdb['train'][0]"
      ],
      "metadata": {
        "id": "BuuEXM0QEP1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imdb['test'][:3]"
      ],
      "metadata": {
        "id": "WGDXhDls-0LU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imdb['train'] = imdb['train'].shuffle(seed=1).select(range(2000))\n",
        "imdb['train']"
      ],
      "metadata": {
        "id": "47Cfe7LvHa58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imdb_train_validation = imdb['train'].train_test_split(train_size=0.8)\n",
        "imdb_train_validation"
      ],
      "metadata": {
        "id": "6T6i3wSa7dnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imdb_train_validation['test']"
      ],
      "metadata": {
        "id": "MwqRoqGw8ceB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imdb_train_validation['validation'] = imdb_train_validation.pop('test')\n",
        "imdb_train_validation"
      ],
      "metadata": {
        "id": "Ekv4pB2X9rfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imdb.update(imdb_train_validation)\n",
        "imdb"
      ],
      "metadata": {
        "id": "IoTQkAy8-Ich"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imdb['test'] = imdb['test'].shuffle(seed=1).select(range(400))\n",
        "imdb['test']"
      ],
      "metadata": {
        "id": "t8yK_crFIPVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imdb['unsupervised'][:3]"
      ],
      "metadata": {
        "id": "cAkJxxdk-iaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imdb.pop('unsupervised')\n",
        "imdb"
      ],
      "metadata": {
        "id": "1V7EsAEt-pk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview of IMDB Dataset"
      ],
      "metadata": {
        "id": "vVWxeSidE6R3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "pd.set_option('max_colwidth', 250)"
      ],
      "metadata": {
        "id": "9W8lNegFGEGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imdb.set_format('pandas')\n",
        "df = imdb['train'][:]\n",
        "df.sample(frac=1 ,random_state=1).head(10)"
      ],
      "metadata": {
        "id": "kYIQ4TbFBF8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.loc[0, 'text']"
      ],
      "metadata": {
        "id": "VNmjEPnCGXKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['text'] = df.text.str.replace('<br />', '')\n",
        "df.loc[0, 'text']"
      ],
      "metadata": {
        "id": "EjA116L-GrHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.label.value_counts()"
      ],
      "metadata": {
        "id": "YAbSUGBPFkGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Words per review\"] = df[\"text\"].str.split().apply(len)\n",
        "df.boxplot(\"Words per review\", by=\"label\", grid=False, showfliers=False,\n",
        "           color=\"black\")\n",
        "plt.suptitle(\"\")\n",
        "plt.xlabel(\"\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iMkhVdcXIYlA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 0 is negative\n",
        "# 1 is positive\n",
        "df[df.text.str.len() < 200]"
      ],
      "metadata": {
        "id": "l8ApthcqFz6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imdb.reset_format()"
      ],
      "metadata": {
        "id": "xjPAmZ4EHis9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizer"
      ],
      "metadata": {
        "id": "w_gtRu5PJgt0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "checkpoint = \"distilbert-base-cased\"\n",
        "#checkpoint = \"bert-base-cased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "def tokenize_function(batch):\n",
        "    return tokenizer(batch[\"text\"], padding=True, truncation=True)\n",
        "\n",
        "imdb_encoded = imdb.map(tokenize_function, batched=True, batch_size=None)\n",
        "imdb_encoded"
      ],
      "metadata": {
        "id": "J0X7XtbYH0p4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(imdb_encoded['train'][0])"
      ],
      "metadata": {
        "id": "uNyIowdLJcnx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tiny IMDB"
      ],
      "metadata": {
        "id": "5gFjGNLZXixQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "import re\n",
        "\n",
        "[x for x in dir(transformers) if re.search(r'^AutoModel', x)]"
      ],
      "metadata": {
        "id": "SUpl1H7qF9KK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "num_labels = 2\n",
        "model = (AutoModelForSequenceClassification\n",
        "         .from_pretrained(checkpoint, num_labels=num_labels)\n",
        "         .to(device))"
      ],
      "metadata": {
        "id": "8Zbgt_1rKVcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import DatasetDict\n",
        "\n",
        "tiny_imdb = DatasetDict()\n",
        "tiny_imdb['train'] = imdb['train'].shuffle(seed=1).select(range(50))\n",
        "tiny_imdb['validation'] = imdb['validation'].shuffle(seed=1).select(range(10))\n",
        "tiny_imdb['test'] = imdb['test'].shuffle(seed=1).select(range(10))\n",
        "\n",
        "tiny_imdb_encoded = tiny_imdb.map(tokenize_function, batched=True, batch_size=None)\n",
        "tiny_imdb_encoded"
      ],
      "metadata": {
        "id": "Lw_n8OP5yNI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "batch_size = 8\n",
        "logging_steps = len(tiny_imdb_encoded[\"train\"]) // batch_size\n",
        "model_name = f\"{checkpoint}-finetuned-tiny-imdb\"\n",
        "training_args = TrainingArguments(output_dir=model_name,\n",
        "                                  num_train_epochs=2,\n",
        "                                  learning_rate=2e-5,\n",
        "                                  per_device_train_batch_size=batch_size,\n",
        "                                  per_device_eval_batch_size=batch_size,\n",
        "                                  weight_decay=0.01,\n",
        "                                  evaluation_strategy=\"epoch\",\n",
        "                                  disable_tqdm=False,\n",
        "                                  logging_steps=logging_steps,\n",
        "                                  log_level=\"error\",\n",
        "                                  optim='adamw_torch'\n",
        "                                  )\n",
        "training_args"
      ],
      "metadata": {
        "id": "Ch0RWXjpXqMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "trainer = Trainer(model=model, \n",
        "                  args=training_args, \n",
        "                  train_dataset=tiny_imdb_encoded[\"train\"],\n",
        "                  eval_dataset=tiny_imdb_encoded[\"validation\"],\n",
        "                  tokenizer=tokenizer)\n",
        "trainer.train();"
      ],
      "metadata": {
        "id": "iKMb2xTN0NJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = trainer.predict(tiny_imdb_encoded['test'])\n",
        "preds"
      ],
      "metadata": {
        "id": "Tsy4dR-7ZAv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds.predictions.shape"
      ],
      "metadata": {
        "id": "F_gLkCEiZRSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds.predictions.argmax(axis=-1)"
      ],
      "metadata": {
        "id": "rhEcbSExZaHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds.label_ids"
      ],
      "metadata": {
        "id": "tsXFMaTaZfHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "accuracy_score(preds.label_ids, preds.predictions.argmax(axis=-1))"
      ],
      "metadata": {
        "id": "mN4SyJHuZjdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_accuracy(preds):\n",
        "  predictions = preds.predictions.argmax(axis=-1)\n",
        "  labels = preds.label_ids\n",
        "  accuracy = accuracy_score(preds.label_ids, preds.predictions.argmax(axis=-1))\n",
        "  return {'accuracy': accuracy}\n"
      ],
      "metadata": {
        "id": "DXKvztDWZ0tQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "trainer = Trainer(model=model, \n",
        "                  compute_metrics=get_accuracy,\n",
        "                  args=training_args, \n",
        "                  train_dataset=tiny_imdb_encoded[\"train\"],\n",
        "                  eval_dataset=tiny_imdb_encoded[\"validation\"],\n",
        "                  tokenizer=tokenizer)\n",
        "trainer.train();"
      ],
      "metadata": {
        "id": "f2ItD2XQag5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training run"
      ],
      "metadata": {
        "id": "bFKPj95ObT89"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "logging_steps = len(imdb_encoded[\"train\"]) // batch_size\n",
        "model_name = f\"{checkpoint}-finetuned-imdb\"\n",
        "training_args = TrainingArguments(output_dir=model_name,\n",
        "                                  num_train_epochs=2,\n",
        "                                  learning_rate=2e-5,\n",
        "                                  per_device_train_batch_size=batch_size,\n",
        "                                  per_device_eval_batch_size=batch_size,\n",
        "                                  weight_decay=0.01,\n",
        "                                  evaluation_strategy=\"epoch\",\n",
        "                                  disable_tqdm=False,\n",
        "                                  logging_steps=logging_steps,\n",
        "                                  log_level=\"error\",\n",
        "                                  optim='adamw_torch'\n",
        "                                  )"
      ],
      "metadata": {
        "id": "VeR9PTxMz-zA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "trainer = Trainer(model=model, \n",
        "                  args=training_args, \n",
        "                  compute_metrics=get_accuracy,\n",
        "                  train_dataset=imdb_encoded[\"train\"],\n",
        "                  eval_dataset=imdb_encoded[\"validation\"],\n",
        "                  tokenizer=tokenizer)\n",
        "trainer.train();"
      ],
      "metadata": {
        "id": "w-mrYweVL_9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "79fBSXBGlkiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model()"
      ],
      "metadata": {
        "id": "C_rZFIbXL73P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name"
      ],
      "metadata": {
        "id": "nbo96MxTAulG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "classifier = pipeline('text-classification', model=model_name)\n",
        "classifier('This is not my idea of fun')"
      ],
      "metadata": {
        "id": "2LGdWc2-MzyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier('This was beyond incredible')"
      ],
      "metadata": {
        "id": "0qniEZcAMCLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L9v0QpQgPrH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ts4dTGCLye-t"
      },
      "source": [
        "# Vision Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PP1Y4hXCypGD"
      },
      "source": [
        "## Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQOPe6LFy88U"
      },
      "source": [
        "### Datasets\n",
        "The CIFAR-10 dataset is a well-known image dataset\n",
        "\n",
        "(If you are not familiar with the datasets library, use this tutorial to help you: https://huggingface.co/docs/datasets/tutorial)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oRr2c9VuyVLY"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "# HINT: Try help(datasets.load_dataset)\n",
        "import datasets\n",
        "\n",
        "cifar = ... # complete this"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qe3oMWJF8GQM"
      },
      "source": [
        "#### Hint\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdLY1aa88J4E"
      },
      "source": [
        "Try help(datasets.load_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bijR3km9875s"
      },
      "source": [
        "### View sample images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvtPirjH038g"
      },
      "source": [
        "Datasets are very similar in structure to python dictionaries.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W1-EDODq-f-y"
      },
      "outputs": [],
      "source": [
        "#Display the first 5 images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-RUJxvr-tQA"
      },
      "source": [
        "#### Hint\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vke-uizj-tQA"
      },
      "source": [
        "Try help(display)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Llpo7BBC_R-d"
      },
      "source": [
        "### Questions on dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlFD0JS5_cya"
      },
      "source": [
        "- How many images are there in the dataset?\n",
        "- What information is available with each sample of the dataset?\n",
        "- How many train samples do we have?\n",
        "- How many validation and test samples do we have?\n",
        "- How many labels are there for this dataset? What are they?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJfJdoe9_UeL"
      },
      "outputs": [],
      "source": [
        "# Create a list, labels, that contains all the different label names\n",
        "labels = ... # Complete this"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XyJ_-nNBvOJ"
      },
      "source": [
        "#### Hint:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z7wm80MvDPtj"
      },
      "outputs": [],
      "source": [
        "cifar['train'].features"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reducing the size of the dataset"
      ],
      "metadata": {
        "id": "RhupFXQEwD5Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reduce the size of the dataset so that we have:\n",
        "- train - 5000\n",
        "- test - 500"
      ],
      "metadata": {
        "id": "fK79iuAzf4tk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#cifar['train'] = ...\n",
        "#cifar['test'] = ..."
      ],
      "metadata": {
        "id": "L17_WkqvgFxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now create a validation dataset, using 20% from the train dataset"
      ],
      "metadata": {
        "id": "wVDdTzgNgB9K"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7y6Z5X7MFBAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5GDc4U2SFA9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zd00fUApCNX7"
      },
      "source": [
        "### Problem: Create a mapping between labels and IDs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOBhDWOrDS5S"
      },
      "source": [
        "Each of the numeric labels has an associated text description."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JTU73kc-CkzK"
      },
      "outputs": [],
      "source": [
        "# Create a mapping between the numeric label and the text description\n",
        "id2label = ... # Complete this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nt9Oa5-jELmn"
      },
      "outputs": [],
      "source": [
        "# Now do the reverse, create a mapping between the text description and the numeric label\n",
        "label2id = ... # Complete this"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Hint:"
      ],
      "metadata": {
        "id": "KHLG0MyEsA8c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider using a dictionary comprehension"
      ],
      "metadata": {
        "id": "0Beu9wJKsJRi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pre-processing images"
      ],
      "metadata": {
        "id": "5acWwCY5uO7O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When working with text data, we need to tokenize the text so that they are in a numerical form that a model can work with. A feature extractor is similar to a tokenizer, but we are not concerned with text. \n",
        "\n",
        "- What are some of the pre-processing steps you might be interested in, when working with images?"
      ],
      "metadata": {
        "id": "x5wEbM4iuXOh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Select a relevant feature extractor\n",
        "checkpoint = 'google/vit-base-patch16-224'\n",
        "\n",
        "feature_extractor = ... # complete this"
      ],
      "metadata": {
        "id": "iWaMW7UquVbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "\n",
        "from torchvision.transforms import (\n",
        "    Compose,\n",
        "    Normalize,\n",
        "    RandomHorizontalFlip,\n",
        "    RandomResizedCrop,\n",
        "    ToTensor,\n",
        "    Resize,\n",
        "    CenterCrop\n",
        ")\n"
      ],
      "metadata": {
        "id": "xybRV_vBvYch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the next section we do a couple of transformations to the images, such as randomly resize the, a horizontal flip etc.\n",
        "- Why do we do these?"
      ],
      "metadata": {
        "id": "5k4RO92pPgeE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform image normalization\n",
        "normalize = ... # Complete this"
      ],
      "metadata": {
        "id": "O0KKDJYlQQdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_transform = Compose(\n",
        "    [\n",
        "     RandomResizedCrop(feature_extractor.size),\n",
        "     RandomHorizontalFlip(),\n",
        "     ToTensor(),\n",
        "     normalize\n",
        "    ]\n",
        ")\n",
        "\n",
        "validation_transform = Compose(\n",
        "        [\n",
        "            Resize(feature_extractor.size),\n",
        "            CenterCrop(feature_extractor.size),\n",
        "            ToTensor(),\n",
        "            normalize,\n",
        "        ]\n",
        "    )\n",
        "\n",
        "def train_transform_images(images):\n",
        "  images[\"pixel_values\"] = [train_transform(image.convert(\"RGB\")) for image in images[\"img\"]]\n",
        "  return images\n",
        "\n",
        "def validation_transform_images(images):\n",
        "  images[\"pixel_values\"] = [validation_transform(image.convert(\"RGB\")) for image in images[\"img\"]]\n",
        "  return images"
      ],
      "metadata": {
        "id": "rodHa4ZMQGEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#transformed_cifar['train'].set_transform(train_transform_images)\n",
        "#transformed_cifar['validation'].set_transform(validation_transform_images)\n",
        "#transformed_cifar['test'].set_transform(validation_transform_images)"
      ],
      "metadata": {
        "id": "kq6GjvIVykvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformed_cifar = cifar.with_transform(train_transform_images)\n",
        "transformed_cifar['train'] = cifar['train'].with_transform(train_transform_images)\n",
        "transformed_cifar['validation'] = cifar['validation'].with_transform(validation_transform_images)\n",
        "transformed_cifar['test'] = cifar['test'].with_transform(validation_transform_images)"
      ],
      "metadata": {
        "id": "RTtu3RNf5MOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformed_cifar['train'][:2]"
      ],
      "metadata": {
        "id": "43Rc3u1t2QtQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A transformed image"
      ],
      "metadata": {
        "id": "SQuIXYu_WQip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display a sample image from your training dataset\n",
        "sample_image = ..."
      ],
      "metadata": {
        "id": "ScZrZZgHT_O6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now apply the transformations and show what this transformed image looks like\n",
        "transformed_sample_image = ..."
      ],
      "metadata": {
        "id": "9YX-hVcOTzbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformed_sample_image.shape"
      ],
      "metadata": {
        "id": "3BYvrODnUeAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_image = cifar['train'][0]['img']\n",
        "sample_image"
      ],
      "metadata": {
        "id": "2BoauXKsYmW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Getting images in the right format"
      ],
      "metadata": {
        "id": "yZmNkpKwmMAO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4-images**"
      ],
      "metadata": {
        "id": "KfmZYJSbnwM3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "four_images = [transformed_cifar['train'][i] for i in range(4)]\n",
        "four_images"
      ],
      "metadata": {
        "id": "Ikhrn_hjmYx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(four_images[0]['pixel_values'].shape, four_images[1]['pixel_values'].shape, four_images[2]['pixel_values'].shape, four_images[3]['pixel_values'].shape)"
      ],
      "metadata": {
        "id": "-rLpZUSboBhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "four_images_labels = [image['label'] for image in four_images]\n",
        "four_images_labels"
      ],
      "metadata": {
        "id": "Exn-nl9fob8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Now we know these need to be converted to tensors"
      ],
      "metadata": {
        "id": "PAAA4qGQpVBs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert these images to tensors\n",
        "#four_images_labels = ..."
      ],
      "metadata": {
        "id": "u4FHTjCoiaPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Now we try and do the same for the pixel_values"
      ],
      "metadata": {
        "id": "r0JmZ5YcqDlz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "four_images_pixel_values = torch.tensor([image['pixel_values'] for image in four_images])\n",
        "four_images_pixel_values"
      ],
      "metadata": {
        "id": "WRBcWLA-qCn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "four_images_pixel_values = torch.cat([image['pixel_values'] for image in four_images])\n",
        "four_images_pixel_values"
      ],
      "metadata": {
        "id": "Ot6CGsw-qi8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "four_images_pixel_values.shape"
      ],
      "metadata": {
        "id": "mgbAJ_xjquB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now get the four_images_pixel_values in the right format\n",
        "four_images_pixel_values = ..."
      ],
      "metadata": {
        "id": "i3hNqvYKjCLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's put this all together in a collate function\n",
        "- Why do we have 'labels' and not 'label' ?"
      ],
      "metadata": {
        "id": "58HuRwGpjQYc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def collate_fn(images):\n",
        "  labels = torch.tensor([image['label'] for image in images])\n",
        "  pixel_values = torch.stack([image['pixel_values'] for image in images])\n",
        "  return {'pixel_values': pixel_values, 'labels': labels}\n",
        "\n",
        "train_dataloader = DataLoader(transformed_cifar['train'], batch_size=4, collate_fn=collate_fn, shuffle=True)\n",
        "validation_dataloader = DataLoader(transformed_cifar['validation'], batch_size=4, collate_fn=collate_fn, shuffle=False)\n",
        "test_dataloader = DataLoader(transformed_cifar['test'], batch_size=4, collate_fn=collate_fn, shuffle=False)"
      ],
      "metadata": {
        "id": "haB_6uiSMluh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = next(iter(train_dataloader))\n",
        "\n",
        "for key, value in batch.items():\n",
        "  print(key, value.shape)"
      ],
      "metadata": {
        "id": "TQbcX4_HH9Bq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using a pre-trained model"
      ],
      "metadata": {
        "id": "NOj7FNVBszBf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When looking at text classification, we used a BERT model which is very similar to a Vision Transformer model. "
      ],
      "metadata": {
        "id": "ZfbKFgVUs7Rb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Select a relevant model\n",
        "checkpoint = 'google/vit-base-patch16-224'\n",
        "\n",
        "model = ... # complete this"
      ],
      "metadata": {
        "id": "RIZPbIoJtVXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training arguments"
      ],
      "metadata": {
        "id": "NP2I3s3j2RfB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "batch_size=32\n",
        "metric_name = \"accuracy\"\n",
        "\n",
        "args = TrainingArguments(\n",
        "    f\"cifar-10\",\n",
        "    save_strategy=\"epoch\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=1,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=metric_name,\n",
        "    logging_dir='logs',\n",
        "    remove_unused_columns=False,\n",
        ")"
      ],
      "metadata": {
        "id": "G0ledBnFor2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compute metrics - accuracy"
      ],
      "metadata": {
        "id": "t0pw9LJZ2YAc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_metric\n",
        "import numpy as np\n",
        "\n",
        "metric = load_metric(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return metric.compute(predictions=predictions, references=labels)"
      ],
      "metadata": {
        "id": "iP3Bvmyhorz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model training"
      ],
      "metadata": {
        "id": "PSaFIXmJ2hth"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "model_name = f\"{checkpoint}-finetuned-cifar10\"\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=transformed_cifar['train'],\n",
        "    eval_dataset=transformed_cifar['validation'],\n",
        "    data_collator=collate_fn,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=feature_extractor\n",
        ")\n",
        "\n",
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "MH9t0lh1orxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs/"
      ],
      "metadata": {
        "id": "riTjz3cGorvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "obc36QQcorsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model()"
      ],
      "metadata": {
        "id": "0Dc9XWIBoYsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference"
      ],
      "metadata": {
        "id": "kcA8ERhKtE5U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "c = load_dataset('cifar10')\n",
        "c"
      ],
      "metadata": {
        "id": "IhE4Rd9nf_Df"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_image = c['test'][-1]['img']\n",
        "test_image"
      ],
      "metadata": {
        "id": "FuaBoiQ7g1cS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "7dbGaH2V1Z09",
        "imrEPOXXxNI_",
        "ncJCvimR5-fQ",
        "m8ptc7LP6ZxD",
        "bFKPj95ObT89",
        "Ts4dTGCLye-t"
      ],
      "name": "O'Reilly Transformers for Computer Vision.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}